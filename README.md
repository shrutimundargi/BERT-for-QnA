# BERT-for-QnA
A simple question answer model using BERT.

BERT is a huge model, with 24 Transformer blocks, 1024 hidden units in each layer, and 340M parameters.
The model is pre-trained on 40 epochs over a 3.3 billion word corpus, including BooksCorpus (800 million words) and English Wikipedia (2.5 billion words).

The goal is to make the model efficient enough to be able to answer every question about 'breads and baking'.
