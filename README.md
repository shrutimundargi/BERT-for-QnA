# BERT-for-QnA
A simple question answer model using BERT.

BERT is a huge model, with 24 Transformer blocks, 1024 hidden units in each layer, and 340M parameters.
The model is pre-trained on 40 epochs over a 3.3 billion word corpus, including BooksCorpus (800 million words) and English Wikipedia (2.5 billion words).
refer (https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) for more information about bert.

The goal here is to make the model efficient enough to be able to answer every question about 'breads and baking'.
